{
  "head": {
    "title": "Building an AI for Weather Predicitions",
    "subtitle": "Building a time series forecasting AI with Keras for the weather",
    "date": 1574013600,
    "link": "building-lstm-neural-network-in-keras-for-weather-data",
    "description": "Building a LSTM neural network in keras has never been eaiser than before. Explore how to use the AI to predict the weather.",
    "image": "image/generic/ai.jpg",
    "markdown": true,
    "tags": [
      "python",
      "ai",
      "deep",
      "machine",
      "learning",
      "numpy",
      "keras",
      "neural",
      "networks",
      "sklearn",
      "lstm",
      "pandas",
      "matplotlib",
      "programming"
    ]
  },
  "contents": "<p>Finally it's time to build our network. In this blog, we will explore different \nparameters to fine tune an AI to predict the weather. This blog is broken into \ntwo other blogs, links are listed below as well.</p>\n<p><a href=\"https://mesonet.agron.iastate.edu/request/download.phtml\" rel=\"noopener noreferrer\" target=\"_blank\">Iowa Environmental Mesonet (ISU)</a></p>\n<p><a href=\"https://github.com/jackmead515/python_ai/tree/master/temperature-in-iowa\" rel=\"noopener noreferrer\" target=\"_blank\">Github Repo</a></p>\n<p><a href=\"http://www.speblog.org/blog/parsing-formatting-weather-data-numpy-pandas-sklearn\" rel=\"noopener noreferrer\" target=\"_blank\">Preparing the Data</a></p>\n<p><a href=\"http://www.speblog.org/blog/visualizing-weather-data-numpy-pandas-sklearn-matplotlib\" rel=\"noopener noreferrer\" target=\"_blank\">Visualizing the Data</a></p>\n<p>Parsing and formatting our dataset can take a while, but building and running our \nneural network can also take a long time. Selecting the layers we need usually doesn't \ntake long at all (if you understand your dataset and the underlying problem to start). \nWhat takes a much longer time is fine tuning all the parameters in your network.</p>\n<p>Good thing is, we already know what kind of network we want to build. That is a LSTM, \nor a Long Short Term Memory network. For reference and further reading, you can learn \nthe exactness of the layer by peeking at the links below.</p>\n<p><a href=\"https://www.youtube.com/watch?v=wY0dyFgNCgY&t=144s\" rel=\"noopener noreferrer\" target=\"_blank\">Jeff Heaton Youtube Channel</a></p>\n<p><a href=\"https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\" rel=\"noopener noreferrer\" target=\"_blank\">Machine Learning Mastery</a></p>\n<p><a href=\"https://keras.io/layers/recurrent/#lstm\" rel=\"noopener noreferrer\" target=\"_blank\">Keras LSTM Documentation</a></p>\n<p>Lets start off by first stacking together the layers of our network.</p>\n<pre><code class=\"python language-python\">from sklearn.metrics import mean_squared_error\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import losses\n\nFEATURES = 1\nWINDOW_SIZE = 24\nNODES = 32\nEPOCHS = 30\nBATCH_SIZE = 100\nOPTIMIZER = optimizers.Adam()\nLOSS = losses.mean_squared_error\n\nmodel = models.Sequential()\nmodel.add(layers.LSTM(NODES, input_shape=(WINDOW_SIZE, FEATURES)))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=OPTIMIZER, loss=LOSS)\n\nhistory = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\nx_pred = model.predict(x_test)\nscore = np.sqrt(mean_squared_error(x_pred, y_test))\nprint('Score (RMSE): {}'.format(score))\n</code></pre>\n<p>It's very easy to throw together a network with little effort. We are choosing to use an LSTM layer for the main guts of this ai. Then we apply a Dense layer with 1 output. This layer will use linear activation to produce one output, our prediction! Then, we compile our model with a loss of mean_squared_error and the adam optimizer. Evaluating the model using it's predict() method on the test set, we can compare the x_pred to the y_test using the mean_squared_error() method from sklearn. This will tell us just how far off our measurements are from the actual value.</p>\n<p>\"Now when I say 'tuning parameters', specifically what I am refering to is these specific parameters: <b>nodes, window_size, optimizer, loss, batch_size, and epochs</b>. Every single one of these parameters should be tuned to create the best network possible.</p>\n<p>We will get back to these parameters in a little bit, but for now, let's have a little fun and just try and run the network over the year 2005. Remember, a WINDOW_SIZE of 1 means that it is roughly 1 hour in the past (we might get a better performing AI if we understand how and when the temperature predictably changes and change the window size accordingly). Below is 5 different rounds with different parameters. View the results of each one!</p>\n<p><details close>\n<summary>Round 1</summary>\n<br></p>\n<pre><code class=\"bash language-bash\">FEATURES = 1\nWINDOW_SIZE = 1\nNODES = 8\nEPOCHS = 100\nBATCH_SIZE = 500\nOPTIMIZER = optimizers.Adam()\nLOSS = losses.mean_squared_error\n\nEpoch 1/100\n8612/8612 [==============================] - 0s 39us/step - loss: 0.1648\nEpoch 2/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.1454\nEpoch 3/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.1294\nEpoch 4/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.1165\nEpoch 5/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.1057\n...\nEpoch 95/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.0022\nEpoch 96/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.0022\nEpoch 97/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.0022\nEpoch 98/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.0022\nEpoch 99/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.0022\nEpoch 100/100\n8612/8612 [==============================] - 0s 4us/step - loss: 0.0022\n\nTraining Time: 4.261866092681885 secs\nTesting Score (RMSE): 1.4618483781814575\n</code></pre>\n<p></details></p>\n<p><details close>\n<summary>Round 2</summary>\n<br></p>\n<pre><code class=\"bash language-bash\">FEATURES = 1\nWINDOW_SIZE = 12\nNODES = 8\nEPOCHS = 100\nBATCH_SIZE = 500\nOPTIMIZER = optimizers.Adam()\nLOSS = losses.mean_squared_error\n\nEpoch 1/100\n8601/8601 [==============================] - 1s 61us/step - loss: 0.2035\nEpoch 2/100\n8601/8601 [==============================] - 0s 16us/step - loss: 0.1393\nEpoch 3/100\n8601/8601 [==============================] - 0s 17us/step - loss: 0.0946\nEpoch 4/100\n8601/8601 [==============================] - 0s 22us/step - loss: 0.0644\nEpoch 5/100\n8601/8601 [==============================] - 0s 17us/step - loss: 0.0443\n...\nEpoch 95/100\n8601/8601 [==============================] - 0s 15us/step - loss: 0.0017\nEpoch 96/100\n8601/8601 [==============================] - 0s 15us/step - loss: 0.0017\nEpoch 97/100\n8601/8601 [==============================] - 0s 15us/step - loss: 0.0017\nEpoch 98/100\n8601/8601 [==============================] - 0s 15us/step - loss: 0.0017\nEpoch 99/100\n8601/8601 [==============================] - 0s 15us/step - loss: 0.0017\nEpoch 100/100\n8601/8601 [==============================] - 0s 15us/step - loss: 0.0017\nTraining Time: 14.28614592552185 secs\nTesting Score (RMSE): 1.3082751035690308\n</code></pre>\n<p></details></p>\n<p><details close>\n<summary>Round 3</summary>\n<br></p>\n<pre><code class=\"bash language-bash\">FEATURES = 1\nWINDOW_SIZE = 24\nNODES = 16\nEPOCHS = 100\nBATCH_SIZE = 500\nOPTIMIZER = optimizers.Adam()\nLOSS = losses.mean_squared_error\n\nEpoch 1/100\n8589/8589 [==============================] - 1s 75us/step - loss: 0.1581\nEpoch 2/100\n8589/8589 [==============================] - 0s 39us/step - loss: 0.0730\nEpoch 3/100\n8589/8589 [==============================] - 0s 42us/step - loss: 0.0409\nEpoch 4/100\n8589/8589 [==============================] - 0s 40us/step - loss: 0.0251\nEpoch 5/100\n8589/8589 [==============================] - 0s 40us/step - loss: 0.0197\n...\nEpoch 95/100\n8589/8589 [==============================] - 0s 40us/step - loss: 0.0015\nEpoch 96/100\n8589/8589 [==============================] - 0s 40us/step - loss: 0.0014\nEpoch 97/100\n8589/8589 [==============================] - 0s 42us/step - loss: 0.0014\nEpoch 98/100\n8589/8589 [==============================] - 0s 41us/step - loss: 0.0014\nEpoch 99/100\n8589/8589 [==============================] - 0s 40us/step - loss: 0.0014\nEpoch 100/100\n8589/8589 [==============================] - 0s 41us/step - loss: 0.0014\nTraining Time: 35.53422498703003 secs\nTesting Score (RMSE): 1.2732785940170288\n</code></pre>\n<p></details></p>\n<p>More to come… Stay tuned…</p>"
}