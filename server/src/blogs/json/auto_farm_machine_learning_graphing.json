{
  "head": {
    "title": "Graphing The Color Masks From Auto Farm Images",
    "subtitle": "Learn how to mangle and wrangle the mask information and shape it to a graphical format using Numpy, Pandas, and Matplotlib",
    "date": 1607407496,
    "link": "auto-farm-machine-learning-pandas-matplotlib-graphing",
    "description": "Using the green masks extracted from the auto farm images, learn how to graph the data to anaylze it over time using Numpy, Pandas, and Matplotlib",
    "image": "image/autofarm/full_green_graph_back.png",
    "markdown": true,
    "tags": [
      "programming",
      "electronics",
      "c++",
      "c",
      "rust",
      "python",
      "javascript",
      "postgres",
      "grafana",
      "raspberry",
      "pi",
      "arduino",
      "opencv",
      "linux",
      "docker",
      "systemd"
    ]
  },
  "contents": "<p>Okay so now we have masks to play around with. What kind of information can we get out of them? For now, until I build a dataset, the best we can do is look at the accumulation of green pixels over time. So lets walk through some fun python tricks to get that.</p>\n<p>Using the <code>get_green_mask()</code> function in the previous blog, we are now creating a new function to total up all the green pixels.</p>\n<pre><code class=\"python language-python\">import cv2\nimport numpy as np\n\ndef get_total_green_pixels(image_file):\n    image = cv2.imread(image_file)\n    if image is not None:\n        green_mask = get_green_mask(image)\n        total_positive = green_mask[green_mask &gt; 0]\n        if len(total_positive) &gt;= 0 and not np.isnan(total_positive).any():\n            return len(total_positive)\n</code></pre>\n<p>The <code>green_mask</code> variable is just a numpy matrix of pixel values. It is zero if there is no green, and 255 if there is a green value. So we fetch all the non zero values from the mask, and return the len of that array giving us the total amount of pixels detected!</p>\n<p>Simple enough, but this function is computationally expensive. Running <code>cv2.imread()</code> is an expensive function as it loads the entire image into a numpy array in memory. Lets write some code to run this function for every single image and make sure we are running it as fast as we can.</p>\n<p>First, understand that we are generating a time series dataset. So lets write a smaller wrapper function to return a data point as <code>[timestamp, green pixels]</code>.</p>\n<pre><code class=\"python language-python\">def run_calculate_green_pixels(image_file):\n    # image_file = \"/path/to/file/123456789.png\"\n    unix_time_sec = int(image_file.split('/')[-1].split('.')[0]) / 1000\n    image_date = datetime.datetime.fromtimestamp(unix_time_sec)\n    total_pixels = get_total_green_pixels(image_file)\n    return [image_date, total_pixels]\n</code></pre>\n<p>Cool! And now lets run this function as fast as we possibly can!</p>\n<pre><code class=\"python language-python\">import numpy as np\nfrom tqdm import tqdm\nfrom multiprocessing import cpu_count\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef calculate_green_pixels(image_group):\n    total_green_pixels = []\n    pool_size = cpu_count()\n    chunks = np.array_split(image_group, len(image_group)/pool_size)\n\n    for index in tqdm(range(len(chunks))):\n        chunk = chunks[index]\n        futures = []\n        pool = ThreadPoolExecutor(pool_size)\n        for image_file in chunk:\n            futures.append(pool.submit(run_calculate_green_pixels, image_file))\n        pool.shutdown(wait=True)\n\n        results = [f.result() for f in futures]\n        total_green_pixels.extend(results)\n\n    numpy_green = np.array(total_green_pixels)\n    return numpy_green\n</code></pre>\n<p>We make use of a <code>ThreadPoolExecutor</code> which will run the IO intensive function <code>cv2.imread()</code> concurrently! And to keep track of the progress, we split the image_group into chunks and use the <code>tqdm</code> library to print the progress. Running the code below in jupyter notebook, here are the results:</p>\n<pre><code class=\"python language-python\">print('calculating 1920x1080...')\npixels_1920x1080 = calculate_green_pixels(image_groups[(1920, 1080)])\nwith open('full_1920x1080_green_55x40x40_80x255x255.npy', 'wb') as f:\n    np.save(f, pixels_1920x1080)\n\nprint('calculating 1280x720...')\npixels_1280x720 = calculate_green_pixels(image_groups[(1280, 720)])\nwith open('full_1280x720_green_55x40x40_80x255x255.npy', 'wb') as f:\n    np.save(f, pixels_1280x720)\n\nprint('calculating 640x480...')\npixels_640x480 = calculate_green_pixels(image_groups[(640, 480)])\nwith open('full_640x480_green_55x40x40_80x255x255.npy', 'wb') as f:\n    np.save(f, pixels_640x480)\n</code></pre>\n<pre><code class=\"shell language-shell\">calculating 1920x1080...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2569/2569 [06:06&lt;00:00,  7.01it/s]\ncalculating 1280x720...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3362/3362 [03:13&lt;00:00, 17.33it/s]\ncalculating 640x480...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [00:14&lt;00:00, 42.10it/s]\n</code></pre>\n<p>Definitely took some time to compute. But, using <code>np.save()</code>, we saved the results to a .npy file to load later and we never have to run this code again!</p>\n<p>Finally, we have our dataset. Keep in mind that this is only one range of green color. If you want to adjust the color range, you'll have to run all of that code again. But, lets just deal with what we got, and graph it!</p>\n<pre><code class=\"python language-python\">import numpy as np\nimport matplotlib.pyplot as plot\n\nfull_1920x1080_green = np.load('full_1920x1080_green_55x40x40_80x255x255.npy', allow_pickle=True)\nfull_1280x720_green = np.load('full_1280x720_green_55x40x40_80x255x255.npy', allow_pickle=True)\nfull_640x480_green = np.load('full_640x480_green_55x40x40_80x255x255.npy', allow_pickle=True)\n\nplot.style.use('default')\nplot.figure(figsize=(15, 6))\nplot.plot(full_1920x1080_green[:, 0], full_1920x1080_green[:, 1], 'b.', label=\"1920x1080\")\nplot.plot(full_1280x720_green[:, 0], full_1280x720_green[:, 1], 'r.', label=\"1280x720\")\nplot.plot(full_640x480_green[:, 0], full_640x480_green[:, 1], 'g.', label=\"640x480\")\nplot.legend(loc=\"upper left\")\nplot.xlabel('Time (year-month-day)')\nplot.ylabel('Total Green Pixels')\nplot.title('Green Pixels of Basil Crop Over Time (28 days)')\nplot.show()\n</code></pre>\n<p>The data is stored as a numpy matrix instead of a pandas dataframe. Why? I dunno. Because I want too? Jeez. The first column in the tensors is the timestamp in milliseconds, the second column is the value. After graphing, we have a pretty clear view of the green pixel information.</p>\n<p><details open>\n<summary>Green Pixel Experiment Results</summary>\n<br>\n    <img src=\"https://www.speblog.org/image/autofarm/full_green_graph_back.png\">\n</details></p>\n<p>Clearly, there is a trend upwards of green data (regardless of the different resolution). But there is also SOOO much noise! Let's try to clear that up.</p>\n<p>Let's focus on a few things. For, lets look at the best resolution images (1080p). Let's also try to aggregate some of the data. It's safe to assume that per hour, the change in green pixels really isn't that different. My basil doesn't grow that fastâ€¦</p>\n<pre><code class=\"python language-python\">import pandas as pd\nimport numpy as np\n\nframe = pd.DataFrame(np.load('full_1920x1080_green_55x40x40_80x255x255.npy', allow_pickle=True))\nframe.columns = ['timestamp', 'value']\n\nframe.timestamp = pd.to_datetime(frame.timestamp, unit='ms')\nframe.value = pd.to_numeric(frame.value)\n\nframe = frame.groupby(pd.Grouper(key='timestamp', freq='1H'))['value'].agg('median')\nframe = frame.to_frame().reset_index()\nframe = frame.dropna()\n\nframe.describe()\n</code></pre>\n<p>We load in the dataset, (yes into a dataframe this time ðŸ˜…). We convert the timestamp column into datetime values and the value column to numeric values. This allows us to do the fancy <code>groupby()</code> aggregation and group all the data points into 1 hour intervals. After selecting the median value, and a few fancy stuff to convert the frame back into a pandas DataFrame, we call <code>describe()</code> to see the overview of results.</p>\n<pre><code class=\"shell language-shell\">count - 286.000000\nmean - 51977.288462\nstd - 14697.838044\nmin - 9249.500000\n25% - 42666.750000\n50% - 50830.250000\n75% - 62301.000000\nmax - 92126.000000\n</code></pre>\n<p>Wow! Reduced <code>20555</code> values into <code>286</code>. Quite a lot. And sure enough, it does seem to reduce a lot of the noise. If we graph it simply, we see this:</p>\n<pre><code class=\"python language-python\">import matplotlib.pyplot as plot\n\nplot.figure(figsize=(15, 6))\nplot.plot(frame.timestamp.values, frame.value.values, 'b.', label=\"1920x1080\")\nplot.legend(loc=\"upper left\")\nplot.xlabel('Time (year-month-day)')\nplot.ylabel('Total Green Pixels')\nplot.title('Green Pixels (Retained only 9am to 4pm)')\nplot.show()\n</code></pre>\n<p><details open>\n<summary>Aggregated One Hour Results</summary>\n<br>\n    <img src=\"https://www.speblog.org/image/autofarm/full_green_graph_early_late_back.png\">\n</details></p>\n<p>And that's just going to about wrap it up for me. In the next blog, I'll go over some different machine learning algorithms along with neural networks that try to show the rate of green pixels growing over time. Until then, I hope you gained somethng from this. If not, let me know in the comments. Stay safe. Peace.</p>"
}